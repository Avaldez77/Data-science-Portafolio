{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake News Classification with RNN vs LSTM (PyTorch)\n",
        "\n",
        "This notebook builds two **recurrent neural network** baselines to classify news articles as **Fake** or **Real**:\n",
        "- a **vanilla RNN**\n",
        "- an **LSTM**\n",
        "\n",
        "The focus is on the full workflow (text cleaning → vectorization → model training → evaluation) and on comparing how\n",
        "a plain RNN behaves versus an LSTM on the same data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preparation\n",
        "\n",
        "Before training any deep learning model, we need the data in a clean, consistent format:\n",
        "- remove noise and normalize text\n",
        "- convert text into numeric representations (tensors)\n",
        "- build train/validation/test splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "We use a dataset of news articles with a binary label:\n",
        "- **Real**\n",
        "- **Fake**\n",
        "\n",
        "Each row includes fields such as `title`, `text`, `subject`, `date`, and `authenticity`.\n",
        "\n",
        "Goal: train sequence models that learn patterns in the text and predict whether the article is fake or real."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ahora sabemos un poco más como es nuestro dataset, creo que el primero que podemos hacer es eliminar puntuación, Upper/Lower case. Al hacer esto podemos estar perdiendo información semántica del texto, pero muchas veces cuando tenemos poder de computo limitado tenemos hacer este trade-off entre acurraccy y tiempo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text preprocessing (NLTK)\n",
        "\n",
        "We apply standard NLP preprocessing to reduce noise and improve signal:\n",
        "- tokenization\n",
        "- stopword removal\n",
        "- lemmatization\n",
        "\n",
        "These steps help the models focus on meaningful words rather than punctuation and filler terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('news.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    donald trump sends embarrassing new year eve m...\n",
              "1    drunk bragging trump staffer started russian c...\n",
              "2    sheriff david clarke becomes internet joke thr...\n",
              "3    trump obsessed even obama name coded website i...\n",
              "4    pope francis called donald trump christmas spe...\n",
              "Name: preprocessado, dtype: object"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "#Limpieza basica del texto, remover puntuación y digitos como fechas, números de usuario de twitter etc.\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+') # Acá es para eligir solo parabras del alfabeto entre A-Z minuscula o mayscula, es una RE\n",
        "lemmatizer = WordNetLemmatizer() #Acá iremos reducir las palabras para su clasificación minima, la raíz semantica de la palabra.\n",
        "\n",
        "\n",
        "primera_noticia = df.iloc[0]\n",
        "def preprocess_text(text):\n",
        "    words = []\n",
        "    for sentence in sent_tokenize(text):\n",
        "        tokens = [word for word in tokenizer.tokenize(sentence)]\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        words += tokens\n",
        "    return ' '.join(words)\n",
        "# Esta funcion nos sirve para etiquetar variables de clasificación binaria\n",
        "\n",
        "tokens_primera_noticia = preprocess_text(primera_noticia['text'])\n",
        "df['texto_titulo'] = df['title'] + df['text']\n",
        "\n",
        "df['preprocessado'] = df['texto_titulo'].apply(preprocess_text)\n",
        "df['preprocessado'].head()\n",
        "# Si queremos palabra por palabra basta hacer .split(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text vectorization (embeddings)\n",
        "\n",
        "Neural networks require numeric inputs. Here we convert tokens into vectors using a pre-trained embedding approach.\n",
        "This keeps the notebook lightweight and avoids training embeddings from scratch (which can be expensive on limited hardware).\n",
        "\n",
        "The final output is a 3D tensor shaped like:\n",
        "`(num_samples, sequence_length, embedding_dim)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(\"glove.6B.100d.txt\")\n",
        "\n",
        "# Function to convert articles to sequences of embeddings\n",
        "def article_to_embedding(article, embeddings_dict, max_len):\n",
        "    embedding_dim = len(next(iter(embeddings_dict.values())))\n",
        "    embedded_article = np.zeros((max_len, embedding_dim))\n",
        "\n",
        "    words = article.split()[:max_len]\n",
        "    for i, word in enumerate(words):\n",
        "        if word in embeddings_dict:\n",
        "            embedded_article[i] = embeddings_dict[word]\n",
        "        else:\n",
        "            embedded_article[i] = np.zeros(embedding_dim)\n",
        "\n",
        "    return embedded_article\n",
        "\n",
        "\n",
        "max_len = 120  # Choose based on dataset analysis\n",
        "embedded_articles = np.array([article_to_embedding(article, glove_embeddings, max_len) for article in df['preprocessado']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10000, 120, 100])"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_as_vectors = torch.as_tensor(embedded_articles, dtype=torch.float)\n",
        "\n",
        "embedding_dim = text_as_vectors.size(2)\n",
        "\n",
        "text_as_vectors.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "df['authenticity_as_num'] = df['authenticity'].apply(lambda x: 0 if x == 'Fake' else 1)\n",
        "labels = torch.as_tensor(df['authenticity_as_num'].values, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(text_as_vectors, labels)\n",
        "\n",
        "# Splitting dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models: RNN and LSTM\n",
        "\n",
        "We train and compare two architectures:\n",
        "\n",
        "- **Vanilla RNN**: simple and fast, but can struggle with long-range dependencies.\n",
        "- **LSTM**: designed to handle longer dependencies and usually trains more stably on sequences.\n",
        "\n",
        "Both models output logits for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        output, _ = self.rnn(x)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Ejemplo de uso: model = SimpleRNN(input_dim=embedding_dim, hidden_dim=128, output_dim=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True) \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, input_size)\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "        # output shape: (batch, seq_len, hidden_dim)\n",
        "\n",
        "        # Take the output of the last time step for classification\n",
        "\n",
        "        output = self.fc(output[:, -1, :])  # shape: (batch, output_dim)\n",
        "        \n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "You can improve results by tuning:\n",
        "- max sequence length\n",
        "- embedding choice\n",
        "- hidden size / number of layers\n",
        "- dropout and learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs, learning_rate):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "           \n",
        "            inputs, labels = batch        \n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs, labels = batch\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Validation Accuracy: {accuracy}%')\n",
        "\n",
        "\n",
        "model = SimpleRNN(input_dim=100, hidden_dim=128, output_dim=2)\n",
        "\n",
        "train_model(model, train_loader, val_loader, epochs=40, learning_rate=  0.001)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\UC-2023.2\\DeepLearning Magister\\ayudantias\\ayudantia05.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/UC-2023.2/DeepLearning%20Magister/ayudantias/ayudantia05.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model2 \u001b[39m=\u001b[39m LSTMModel(input_dim\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, hidden_dim\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, output_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/UC-2023.2/DeepLearning%20Magister/ayudantias/ayudantia05.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_model(model2, train_loader, val_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n",
            "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\UC-2023.2\\DeepLearning Magister\\ayudantias\\ayudantia05.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/UC-2023.2/DeepLearning%20Magister/ayudantias/ayudantia05.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/UC-2023.2/DeepLearning%20Magister/ayudantias/ayudantia05.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/UC-2023.2/DeepLearning%20Magister/ayudantias/ayudantia05.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/UC-2023.2/DeepLearning%20Magister/ayudantias/ayudantia05.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/UC-2023.2/DeepLearning%20Magister/ayudantias/ayudantia05.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "model2 = LSTMModel(input_dim=100, hidden_dim=256, output_dim=2)\n",
        "train_model(model2, train_loader, val_loader, epochs=10, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}