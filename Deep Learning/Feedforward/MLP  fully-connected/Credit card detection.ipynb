{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Credit Card Fraud Detection with a PyTorch MLP\n\nThis notebook is a **portfolio-style** example of building a **binary classifier** for fraud detection using a\n**multi-layer perceptron (MLP)** in **PyTorch**.\n\nIt includes:\n- data loading and basic cleaning (pandas)\n- train/validation/test splits (stratified)\n- feature scaling (scikit-learn)\n- an MLP with BatchNorm + Dropout\n- training with **SGD** vs **Adam**\n- evaluation with precision/recall/F1\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Setup\nIf you're running locally and missing packages, uncomment the install cell.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Uncomment if needed:\n# !pip install -q pandas numpy matplotlib scikit-learn seaborn torch\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\nfrom torch.utils.data import TensorDataset, DataLoader\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data access\nYou can load the CSV from disk, or download it from Kaggle.\n\n### Option A — Local file\nPlace the CSV in the same folder and set `file_path` accordingly.\n\n### Option B — Kaggle download (optional)\nIf you want a reproducible download step, you can use the Kaggle API. That requires a Kaggle token.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Optional Kaggle download (requires credentials configured in your environment)\n# !pip install -q kaggle\n# !kaggle datasets download -d gzdekzlkaya/credit-card-fraud-detection-dataset\n# !unzip -o \"credit-card-fraud-detection-dataset.zip\"\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "file_path = \"creditcard_fraud_detection.csv\"  # update if needed\ndf = pd.read_csv(file_path)\ndf.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.info()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Quick exploration\nFraud datasets are typically **highly imbalanced** (very few positives). We'll inspect the class distribution.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "target_col = \"Class\"  # typical for the popular credit card fraud dataset\ndf[target_col].value_counts().rename({0: \"Legit\", 1: \"Fraud\"})\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(5,4))\ndf[target_col].value_counts().sort_index().plot(kind=\"bar\")\nplt.title(\"Class distribution (0=legit, 1=fraud)\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Split and scale (no leakage)\nWe:\n1. Split into train/validation/test using **stratification**.\n2. Fit the scaler on **train only**, then transform validation/test.\n\nNote: In the classic dataset, features `V1..V28` are already scaled, while `Time` and `Amount` are not.\nScaling everything consistently is still fine for an MLP.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X = df.drop(columns=[target_col])\ny = df[target_col].astype(int)\n\n# 80% train, 10% validation, 10% test\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, train_size=0.8, stratify=y, random_state=42\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, train_size=0.5, stratify=y_temp, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_val_s   = scaler.transform(X_val)\nX_test_s  = scaler.transform(X_test)\n\nX_train_s.shape, X_val_s.shape, X_test_s.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) DataLoaders\nWe build PyTorch DataLoaders for mini-batch training.\n\nFor binary classification with `BCEWithLogitsLoss`, the target is float with shape `(N, 1)`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def to_loader(X_np, y_series, batch_size=2048, shuffle=False):\n    X_tensor = torch.tensor(X_np.astype(np.float32))\n    y_tensor = torch.tensor(y_series.values.astype(np.float32)).unsqueeze(1)\n    ds = TensorDataset(X_tensor, y_tensor)\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\nbatch_size = 2048\ntrain_dl = to_loader(X_train_s, y_train, batch_size=batch_size, shuffle=True)\nval_dl   = to_loader(X_val_s, y_val, batch_size=batch_size, shuffle=False)\ntest_dl  = to_loader(X_test_s, y_test, batch_size=batch_size, shuffle=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Model: MLP with BatchNorm + Dropout\nWe output **logits** (no final sigmoid). Sigmoid is applied only for predictions/metrics.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class FraudMLP(nn.Module):\n    def __init__(self, input_size, hidden=(256, 128, 64), p_drop=0.3):\n        super().__init__()\n        layers = []\n        prev = input_size\n        for h in hidden:\n            layers += [\n                nn.Linear(prev, h),\n                nn.BatchNorm1d(h),\n                nn.ReLU(),\n                nn.Dropout(p_drop),\n            ]\n            prev = h\n        layers += [nn.Linear(prev, 1)]  # logits\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Training utilities (weighted loss for imbalance)\nWith imbalanced targets, a weighted loss helps the model focus on the minority class.\n\nFor `BCEWithLogitsLoss`, you can set:\n`pos_weight = (#negative / #positive)`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def compute_pos_weight(y_series, device):\n    counts = y_series.value_counts().to_dict()\n    n_pos = counts.get(1, 0)\n    n_neg = counts.get(0, 0)\n    if n_pos == 0:\n        return torch.tensor([1.0], device=device)\n    return torch.tensor([n_neg / n_pos], device=device)\n\ndef batch_metrics_from_logits(logits, y_true):\n    probs = torch.sigmoid(logits)\n    preds = (probs >= 0.5).float()\n    acc = (preds == y_true).float().mean().item()\n    return acc\n\ndef run_epoch(model, dl, loss_fn, optimizer=None):\n    is_train = optimizer is not None\n    model.train() if is_train else model.eval()\n\n    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n    for Xb, yb in dl:\n        Xb, yb = Xb.to(device), yb.to(device)\n        logits = model(Xb)\n        loss = loss_fn(logits, yb)\n\n        if is_train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        total_loss += loss.item()\n        total_acc += batch_metrics_from_logits(logits, yb)\n        n_batches += 1\n\n    return total_loss / n_batches, total_acc / n_batches\n\ndef train_model(model, train_dl, val_dl, loss_fn, optimizer, epochs=20, print_every=5):\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_acc = run_epoch(model, train_dl, loss_fn, optimizer=optimizer)\n        va_loss, va_acc = run_epoch(model, val_dl, loss_fn, optimizer=None)\n\n        train_losses.append(tr_loss); val_losses.append(va_loss)\n        train_accs.append(tr_acc);   val_accs.append(va_acc)\n\n        if epoch == 1 or epoch == epochs or epoch % print_every == 0:\n            print(f\"Epoch {epoch:>3} | train loss {tr_loss:.5f} acc {tr_acc:.3f} | val loss {va_loss:.5f} acc {va_acc:.3f}\")\n\n    return train_losses, val_losses, train_accs, val_accs\n\ndef plot_curves(train_vals, val_vals, title, ylabel):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_vals, label=\"Train\")\n    plt.plot(val_vals, label=\"Validation\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef get_predictions(model, dl):\n    model.eval()\n    ys, preds = [], []\n    probs_all = []\n    with torch.no_grad():\n        for Xb, yb in dl:\n            Xb = Xb.to(device)\n            logits = model(Xb)\n            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n            pred = (probs >= 0.5).astype(int)\n\n            probs_all.extend(list(probs))\n            preds.extend(list(pred))\n            ys.extend(list(yb.cpu().numpy().reshape(-1).astype(int)))\n\n    return np.array(ys), np.array(preds), np.array(probs_all)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Train with SGD (baseline)\nSGD can work well with tuning (learning rate, momentum, schedules). We'll start with a reasonable baseline.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "input_size = X_train_s.shape[1]\n\npos_weight = compute_pos_weight(y_train, device)\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\nmodel_sgd = FraudMLP(input_size=input_size, hidden=(256, 128, 64), p_drop=0.3).to(device)\noptimizer_sgd = SGD(model_sgd.parameters(), lr=0.01, momentum=0.9)\n\nepochs = 15\ntr_l, va_l, tr_a, va_a = train_model(model_sgd, train_dl, val_dl, loss_fn, optimizer_sgd, epochs=epochs, print_every=5)\n\nplot_curves(tr_l, va_l, \"Loss (SGD)\", \"BCEWithLogitsLoss\")\nplot_curves(tr_a, va_a, \"Accuracy (SGD)\", \"Accuracy\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "y_true, y_pred, _ = get_predictions(model_sgd, test_dl)\nprint(classification_report(y_true, y_pred, digits=4))\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(4,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\nplt.title(\"Confusion Matrix (SGD)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Train with Adam\nAdam often converges quickly on MLPs with less tuning.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "model_adam = FraudMLP(input_size=input_size, hidden=(256, 128, 64), p_drop=0.3).to(device)\noptimizer_adam = Adam(model_adam.parameters(), lr=0.001)\n\nepochs = 10\ntr_l2, va_l2, tr_a2, va_a2 = train_model(model_adam, train_dl, val_dl, loss_fn, optimizer_adam, epochs=epochs, print_every=5)\n\nplot_curves(tr_l2, va_l2, \"Loss (Adam)\", \"BCEWithLogitsLoss\")\nplot_curves(tr_a2, va_a2, \"Accuracy (Adam)\", \"Accuracy\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "y_true2, y_pred2, _ = get_predictions(model_adam, test_dl)\nprint(classification_report(y_true2, y_pred2, digits=4))\n\ncm2 = confusion_matrix(y_true2, y_pred2)\nplt.figure(figsize=(4,4))\nsns.heatmap(cm2, annot=True, fmt=\"d\", cbar=False)\nplt.title(\"Confusion Matrix (Adam)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notes\n- For fraud detection, **recall** on the fraud class is often more important than raw accuracy.\n- Consider tuning the decision threshold (e.g., 0.2 instead of 0.5) and tracking PR-AUC.\n- You can also try resampling strategies (undersampling/oversampling) or focal loss.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}