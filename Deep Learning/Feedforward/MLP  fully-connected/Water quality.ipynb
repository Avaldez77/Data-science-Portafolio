{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Water Quality Safety Classifier (PyTorch MLP)\n\nA compact end-to-end workflow for **tabular binary classification** using a multi-layer perceptron (MLP) in **PyTorch**.\nThe notebook covers: data cleaning, normalization, train/val/test splits, model variants with BatchNorm + Dropout,\ntraining with SGD vs Adam, and evaluation with classification metrics.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Setup\n\nIf you're running this locally and don't have the libraries installed, uncomment the next cell.\nIn most notebook environments, these are already available.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Uncomment if needed:\n# !pip install -q pandas numpy matplotlib scikit-learn seaborn torch\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Load and clean the dataset\n\nThis dataset is expected to be a CSV file with a binary target column named `is_safe` (1 = safe, 0 = not safe).\n\n**How to run this notebook**\n- Put `waterQuality1.csv` in the same directory as this notebook, or\n- Set `file_path` to the correct location.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "file_path = \"waterQuality1.csv\"  # update if needed\n\ndf = pd.read_csv(file_path)\ndf.head()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Basic cleaning: replace placeholder strings with NaN, then drop rows with missing values\ndf.replace(\"#NUM!\", np.nan, inplace=True)\ndf.dropna(inplace=True)\n\n# Ensure numeric dtype where possible\nfor col in df.columns:\n    if col != \"is_safe\":\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\ndf.dropna(inplace=True)  # drop any rows that became NaN after coercion\n\ndf.info()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Quick EDA\n- Histograms for numeric features\n- Target distribution (often imbalanced)\n- Correlation heatmap (optional, just for intuition)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df.hist(figsize=(15, 10))\nplt.tight_layout()\nplt.show()\n\ndf[\"is_safe\"].hist(figsize=(5, 4))\nplt.title(\"Target distribution: is_safe\")\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(12, 10))\nsns.heatmap(df.corr(numeric_only=True), annot=False, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Train/Validation/Test split + normalization\n\nWe split first, then fit the scaler **only on the training set** to avoid leakage.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X = df.drop(columns=[\"is_safe\"])\ny = df[\"is_safe\"].astype(int)\n\n# Split: 80% train, 10% val, 10% test (stratified)\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, train_size=0.8, stratify=y, random_state=42\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, train_size=0.5, stratify=y_temp, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_s = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\nX_val_s   = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\nX_test_s  = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n\nX_train_s.describe().T.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) DataLoaders\n\nWe convert pandas dataframes into PyTorch tensors and build DataLoaders.\nFor binary classification with `BCEWithLogitsLoss`, targets are floats with shape `(N, 1)`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def to_dataset(X_df, y_series):\n    X_tensor = torch.tensor(X_df.values.astype(np.float32))\n    y_tensor = torch.tensor(y_series.values.astype(np.float32)).unsqueeze(1)\n    return TensorDataset(X_tensor, y_tensor)\n\nbatch_size = 128\n\ntrain_ds = to_dataset(X_train_s, y_train)\nval_ds   = to_dataset(X_val_s, y_val)\ntest_ds  = to_dataset(X_test_s, y_test)\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\ntest_dl  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Model definitions (MLP variants)\n\nWe implement three common variants for tabular data:\n1) **BatchNorm + ReLU**\n2) **BatchNorm + ReLU (deeper)**\n3) **BatchNorm + ReLU + Dropout** (regularization)\n\nAll models output **logits** (no sigmoid at the end). Sigmoid is handled inside the loss or for evaluation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class MLP_BN(nn.Module):\n    def __init__(self, input_size, hidden=(128, 64, 32, 16), output_size=1):\n        super().__init__()\n        layers = []\n        prev = input_size\n        for h in hidden:\n            layers += [\n                nn.Linear(prev, h),\n                nn.BatchNorm1d(h),\n                nn.ReLU(),\n            ]\n            prev = h\n        layers += [nn.Linear(prev, output_size)]  # logits\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\nclass MLP_BN_Dropout(nn.Module):\n    def __init__(self, input_size, hidden=(256, 128, 64, 32), p_drop=0.4, output_size=1):\n        super().__init__()\n        layers = []\n        prev = input_size\n        for h in hidden:\n            layers += [\n                nn.Linear(prev, h),\n                nn.BatchNorm1d(h),\n                nn.ReLU(),\n                nn.Dropout(p_drop),\n            ]\n            prev = h\n        layers += [nn.Linear(prev, output_size)]  # logits\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Training + evaluation utilities\n\n- We use **BCEWithLogitsLoss** for numerical stability (it combines sigmoid + BCE).\n- We compute `pos_weight` to compensate class imbalance: `pos_weight = (#neg / #pos)`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def compute_pos_weight(y_series, device):\n    # y is 0/1 ints\n    counts = y_series.value_counts().to_dict()\n    n_pos = counts.get(1, 0)\n    n_neg = counts.get(0, 0)\n    if n_pos == 0:\n        return torch.tensor([1.0], device=device)\n    return torch.tensor([n_neg / n_pos], device=device)\n\ndef batch_accuracy_from_logits(logits, y_true):\n    probs = torch.sigmoid(logits)\n    preds = (probs >= 0.5).float()\n    return (preds == y_true).float().mean().item()\n\ndef run_epoch(model, dl, loss_fn, optimizer=None):\n    training = optimizer is not None\n    model.train() if training else model.eval()\n\n    total_loss = 0.0\n    total_acc = 0.0\n    n_batches = 0\n\n    for Xb, yb in dl:\n        Xb = Xb.to(device)\n        yb = yb.to(device)\n\n        logits = model(Xb)\n        loss = loss_fn(logits, yb)\n\n        if training:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        total_loss += loss.item()\n        total_acc += batch_accuracy_from_logits(logits, yb)\n        n_batches += 1\n\n    return total_loss / n_batches, total_acc / n_batches\n\ndef train_model(model, train_dl, val_dl, loss_fn, optimizer, epochs=40, print_every=10):\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for epoch in range(1, epochs + 1):\n        tr_loss, tr_acc = run_epoch(model, train_dl, loss_fn, optimizer=optimizer)\n        va_loss, va_acc = run_epoch(model, val_dl, loss_fn, optimizer=None)\n\n        train_losses.append(tr_loss); val_losses.append(va_loss)\n        train_accs.append(tr_acc);   val_accs.append(va_acc)\n\n        if epoch % print_every == 0 or epoch == 1 or epoch == epochs:\n            print(f\"Epoch {epoch:>3} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {va_loss:.4f} acc {va_acc:.3f}\")\n\n    return train_losses, val_losses, train_accs, val_accs\n\ndef plot_curves(train_vals, val_vals, title, ylabel):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_vals, label=\"Train\")\n    plt.plot(val_vals, label=\"Validation\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef get_predictions(model, dl):\n    model.eval()\n    all_y = []\n    all_pred = []\n    with torch.no_grad():\n        for Xb, yb in dl:\n            Xb = Xb.to(device)\n            logits = model(Xb)\n            probs = torch.sigmoid(logits)\n            preds = (probs >= 0.5).long().cpu().numpy().reshape(-1)\n            all_pred.extend(list(preds))\n            all_y.extend(list(yb.cpu().numpy().reshape(-1).astype(int)))\n    return np.array(all_y), np.array(all_pred)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Train with SGD\n\nThis uses momentum and a moderate learning rate. You can tune:\n- `lr`\n- `momentum`\n- architecture depth/width\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "input_size = X_train_s.shape[1]\n\npos_weight = compute_pos_weight(y_train, device)\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\nmodel_sgd = MLP_BN_Dropout(input_size=input_size, p_drop=0.4).to(device)\noptimizer_sgd = SGD(model_sgd.parameters(), lr=0.01, momentum=0.9)\n\nepochs = 60\ntrain_losses, val_losses, train_accs, val_accs = train_model(\n    model_sgd, train_dl, val_dl, loss_fn, optimizer_sgd, epochs=epochs, print_every=10\n)\n\nplot_curves(train_losses, val_losses, \"Loss (SGD)\", \"BCEWithLogitsLoss\")\nplot_curves(train_accs, val_accs, \"Accuracy (SGD)\", \"Accuracy\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "y_true, y_pred = get_predictions(model_sgd, test_dl)\nprint(classification_report(y_true, y_pred, digits=4))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Train with Adam\n\nAdam typically converges faster on tabular MLPs with less tuning.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "model_adam = MLP_BN_Dropout(input_size=input_size, p_drop=0.4).to(device)\noptimizer_adam = Adam(model_adam.parameters(), lr=0.001)\n\nepochs = 40\ntrain_losses_a, val_losses_a, train_accs_a, val_accs_a = train_model(\n    model_adam, train_dl, val_dl, loss_fn, optimizer_adam, epochs=epochs, print_every=10\n)\n\nplot_curves(train_losses_a, val_losses_a, \"Loss (Adam)\", \"BCEWithLogitsLoss\")\nplot_curves(train_accs_a, val_accs_a, \"Accuracy (Adam)\", \"Accuracy\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "y_true_a, y_pred_a = get_predictions(model_adam, test_dl)\nprint(classification_report(y_true_a, y_pred_a, digits=4))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notes and next steps\n- If the dataset is highly imbalanced, accuracy alone can be misleading. Use **F1**, **recall**, and **AUC** when possible.\n- Try:\n  - different `pos_weight`\n  - wider/deeper networks\n  - different dropout (`p_drop`)\n  - early stopping based on validation loss\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}