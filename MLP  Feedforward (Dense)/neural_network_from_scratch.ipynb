{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Neural Network From Scratch (NumPy)\n\nA compact, self-contained walkthrough of building and training a tiny neural network using **only NumPy**.\nIt includes forward pass, backpropagation, and gradient descent updates, plus a few demo tasks.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Setup\nWe use NumPy for vectorized math and Matplotlib for quick visual checks.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nnp.set_printoptions(suppress=True)  # avoid scientific notation in prints\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # required for 3D plots\n\nnp.random.seed(10)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Small helpers\n- `add_dimension_optional` ensures consistent shapes.\n- `visualize_loss` plots training loss.\n- `visualize_preds` compares a learned surface vs the real function.\n- `get_accuracy` computes binary accuracy (threshold at 0.5).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def add_dimension_optional(arr):\n    new_arr = arr\n    if new_arr.ndim == 1:\n        new_arr = arr[:, None]\n    return new_arr\n\ndef visualize_loss(loss_history):\n    plt.plot(loss_history)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss\")\n    plt.show()\n\ndef visualize_preds(model):\n    # Grid over (x1, x2)\n    x1 = np.linspace(-1, 1, 50)\n    x2 = np.linspace(-1, 1, 50)\n    X1, X2 = np.meshgrid(x1, x2)\n    X_grid = np.column_stack((X1.ravel(), X2.ravel()))\n\n    # Model prediction on the grid\n    y_pred = model.forward(X_grid).reshape(X1.shape)\n\n    # True function for comparison\n    y_true = (X1**2 + X2**2)\n\n    fig = plt.figure(figsize=(12, 5))\n\n    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n    ax1.plot_surface(X1, X2, y_true)\n    ax1.set_title(r\"True Function: $x_1^2 + x_2^2$\")\n    ax1.set_xlabel(r\"$x_1$\")\n    ax1.set_ylabel(r\"$x_2$\")\n    ax1.set_zlabel(r\"$y$\")\n\n    ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n    ax2.plot_surface(X1, X2, y_pred)\n    ax2.set_title(\"Model Prediction\")\n    ax2.set_xlabel(r\"$x_1$\")\n    ax2.set_ylabel(r\"$x_2$\")\n    ax2.set_zlabel(\"Prediction\")\n\n    plt.tight_layout()\n    plt.show()\n\ndef get_accuracy(net, X, y):\n    y_pred = net.forward(X)\n    y_pred_bin = np.where(y_pred > 0.5, 1, 0)\n    return np.sum(y_pred_bin == y) / len(y)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Activation functions\nActivation functions add non-linearity. We'll use:\n- **Identity** (useful for output layers in regression)\n- **Sigmoid** (classic for binary classification demos)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class Identity:\n    def forward(self, x):\n        return x\n\n    def backward(self, x):\n        return np.ones_like(x)\n\nclass Sigmoid:\n    def forward(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def backward(self, x):\n        f = self.forward(x)\n        return f * (1 - f)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Loss function (MSE)\nMean Squared Error is common for regression and simple experiments:\n\\begin{align}\n\\text{MSE}(y,\\hat{y})=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\n\\end{align}\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class MSE:\n    def forward(self, y_pred, y):\n        return np.mean((y_pred - y) ** 2)\n\n    def backward(self, y_pred, y):\n        return 2 * (y_pred - y)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Core building blocks: Neuron, Layer, Network\n\n- A **Neuron** computes: `z = x·w + b`, then applies an activation function.\n- A **Layer** is a collection of neurons working in parallel.\n- A **NeuralNetwork** is a stack of layers.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class Neuron:\n    def __init__(self, n_inputs, activation=Identity()):\n        self.activation = activation\n        self.w = 0.1 * np.random.randn(n_inputs).reshape(n_inputs, -1)\n        self.b = 1.0\n        self.z = None\n        self.db = None\n        self.dw = None\n\n    def forward(self, x):\n        # Linear part + activation\n        self.z = np.dot(x, self.w) + self.b\n        a = self.activation.forward(self.z)\n        return a\n\n    def backward(self, da, a_prev):\n        da = add_dimension_optional(da)\n        dz = da * self.activation.backward(self.z)\n\n        # Gradients\n        self.dw = np.dot(a_prev.T, dz)\n        self.db = np.sum(dz, axis=0, keepdims=True)\n\n        # Gradient for previous layer\n        da_prev = np.dot(dz, self.w.T)\n        return da_prev\n\n    def step(self, l_r):\n        # Gradient descent update\n        self.w -= l_r * self.dw\n        self.b -= l_r * self.db\n\nclass Layer:\n    def __init__(self, n_inputs, n_neurons, activation=Identity()):\n        self.neurons = [Neuron(n_inputs, activation) for _ in range(n_neurons)]\n        self.A = None  # layer activations\n\n    def forward(self, x):\n        self.A = np.column_stack([n.forward(x) for n in self.neurons])\n        return self.A\n\n    def backward(self, da, a_prev):\n        da_prev = np.array([n.backward(da[:, i], a_prev) for i, n in enumerate(self.neurons)])\n        da_prev = np.sum(da_prev, axis=0)\n        return da_prev\n\n    def step(self, l_r):\n        for n in self.neurons:\n            n.step(l_r)\n\nclass NeuralNetwork:\n    def __init__(self, layer_sizes, activation=Identity()):\n        self.layers = []\n        self._init_layers(layer_sizes, activation)\n\n    def _init_layers(self, layer_sizes, activation):\n        for n_in, n_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n            self.layers.append(Layer(n_in, n_out, activation))\n\n    def forward(self, x):\n        out = x\n        for layer in self.layers:\n            out = layer.forward(out)\n        return out\n\n    def backward(self, X, da):\n        grad = da\n        for i, layer in reversed(list(enumerate(self.layers))):\n            a_prev = X if i == 0 else self.layers[i - 1].A\n            grad = layer.backward(grad, a_prev)\n        return grad\n\n    def step(self, l_r):\n        for layer in self.layers:\n            layer.step(l_r)\n\n    def predict(self, x):\n        return self.forward(x)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Training loop\nA minimal supervised learning loop:\n1) forward pass → predictions  \n2) loss evaluation  \n3) backward pass → gradients  \n4) parameter update\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def train(net, X, y, epochs, l_r, l_f):\n    loss_history = []\n    for _ in range(epochs):\n        y_pred = net.forward(X)\n        loss = l_f.forward(y_pred, y)\n        loss_history.append(loss)\n\n        da = l_f.backward(y_pred, y)\n        net.backward(X, da)\n        net.step(l_r)\n\n    return loss_history\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Demo 1: Learn AND (binary classification)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([[0], [0], [0], [1]])\n\nnet = NeuralNetwork([2, 1], activation=Sigmoid())\n\nprint(\"Before training\")\nprint(\"Predictions:\")\nprint(net.forward(X))\nprint(f\"Accuracy: {100*get_accuracy(net, X, y):.1f}%\")\n\nloss_history = train(net, X, y, epochs=1000, l_r=0.5, l_f=MSE())\n\nprint(\"\\nAfter training\")\nprint(\"Predictions:\")\nprint(net.forward(X))\nprint(f\"Accuracy: {100*get_accuracy(net, X, y):.1f}%\")\n\nvisualize_loss(loss_history)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Demo 2: Learn XOR (requires a hidden layer)\nXOR is not linearly separable, so we use a small network with a hidden layer.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([[0], [1], [1], [0]])\n\nnet = NeuralNetwork([2, 2, 1], activation=Sigmoid())\nloss_history = train(net, X, y, epochs=10000, l_r=0.1, l_f=MSE())\n\nprint(\"Final predictions (XOR):\")\nprint(net.predict(X))\nprint(f\"Accuracy: {100*get_accuracy(net, X, y):.1f}%\")\n\nvisualize_loss(loss_history)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Demo 3: Approximate a smooth function\nWe fit:\n\\begin{align}\nF(x_1, x_2) = x_1^2 + x_2^2\n\\end{align}\non random samples in [-1, 1].\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Generate a small training set\nX = np.array([[np.random.uniform(-1, 1), np.random.uniform(-1, 1)] for _ in range(30)])\ny = np.array([[x[0]**2 + x[1]**2] for x in X])\n\nnet = NeuralNetwork([2, 10, 1], activation=Sigmoid())\nloss_history = train(net, X, y, epochs=5000, l_r=0.01, l_f=MSE())\n\nprint(\"A few examples (input → prediction vs true):\")\nfor i in range(5):\n    x_i = X[i].reshape(1, 2)\n    y_pred_i = net.predict(x_i)[0, 0]\n    print(f\"{x_i.flatten()} → {y_pred_i:.4f}  vs  {y[i,0]:.4f}\")\n\nvisualize_loss(loss_history)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visual check (3D surface)\nThis compares the true function surface against the learned approximation on a grid.\nIf your surface looks off, re-run training (random initialization can change convergence).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "visualize_preds(net)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}